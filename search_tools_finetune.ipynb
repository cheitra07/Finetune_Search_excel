```python
# Step 1: Install dependencies
!pip install -q transformers datasets

# Step 2: Load dataset
import pandas as pd

data = {
    "text": [
        "Engine noise on startup",
        "Screen not turning on",
        "Oil leakage from valve",
        "Engine makes rattling noise",
        "Display flickers randomly",
        "Oil leak near gasket",
    ],
    "label": [
        "engine_issue",
        "display_issue",
        "oil_leak",
        "engine_issue",
        "display_issue",
        "oil_leak"
    ]
}

df = pd.DataFrame(data)

# Step 3: Encode labels
label2id = {"engine_issue": 0, "display_issue": 1, "oil_leak": 2}
df["label"] = df["label"].map(label2id)

# Step 4: Train-test split
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.33, random_state=42)

# Step 5: Load tokenizer and model
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Step 6: Tokenize datasets
from datasets import Dataset

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

train_dataset = train_dataset.map(tokenize)
test_dataset = test_dataset.map(tokenize)

# Step 7: Define metrics
from sklearn.metrics import accuracy_score
def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    return {"accuracy": accuracy_score(p.label_ids, preds)}

# Step 8: Set training arguments
from transformers import TrainingArguments, Trainer
import os
os.environ["WANDB_DISABLED"] = "true"  # disables wandb

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=5,
    evaluation_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="no"
)

# Step 9: Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

# Step 10: Test prediction
text = "Oil leak found under machine"
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs)
predicted = outputs.logits.argmax().item()
id2label = {v: k for k, v in label2id.items()}
print(f"Predicted label: {id2label[predicted]}")
```
model_path = "./fine_tuned_distilbert"
trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)

import shutil
shutil.make_archive("fine_tuned_distilbert", 'zip', model_path)

from google.colab import files
files.download("fine_tuned_distilbert.zip")
